{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de4e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b472e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import librosa\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tabulate import tabulate\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b698af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf906a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandsDataset(Dataset):\n",
    "    \"\"\"Google Speech Commands dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split,transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the data files.\n",
    "            split    (string): In [\"train\", \"valid\", \"test\"].\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.number_of_classes = len(self.get_classes())\n",
    "\n",
    "        self.class_to_file = defaultdict(list)\n",
    "\n",
    "        self.valid_filenames = self.get_valid_filenames()\n",
    "        self.test_filenames = self.get_test_filenames()\n",
    "\n",
    "        for c in self.get_classes():\n",
    "            file_name_list = sorted(os.listdir(self.root_dir + \"dataset/\" + c))\n",
    "            for filename in file_name_list:\n",
    "                if split == \"train\":\n",
    "                    if (filename not in self.valid_filenames[c]) and (filename not in self.test_filenames[c]):\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                elif split == \"valid\":\n",
    "                    if filename in self.valid_filenames[c]:\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                elif split == \"test\":\n",
    "                    if filename in self.test_filenames[c]:\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid split name.\")\n",
    "\n",
    "        self.filepath_list = list()\n",
    "        self.label_list = list()\n",
    "        for cc, c in enumerate(self.get_classes()):\n",
    "            f_extension = sorted(list(self.class_to_file[c]))\n",
    "            l_extension = [cc for i in f_extension]\n",
    "            f_extension = [self.root_dir + \"dataset/\" + c + \"/\" + filename for filename in f_extension]\n",
    "            self.filepath_list.extend(f_extension)\n",
    "            self.label_list.extend(l_extension)\n",
    "        self.number_of_samples = len(self.filepath_list)\n",
    "    def __len__(self):\n",
    "        return self.number_of_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = np.zeros((16000, ), dtype=np.float32)\n",
    "\n",
    "        sample_file = self.filepath_list[idx]\n",
    "\n",
    "        sample_from_file = read(sample_file)[1]\n",
    "        sample[:sample_from_file.size] = sample_from_file\n",
    "        sample = sample.reshape((16000, ))\n",
    "        \n",
    "        sample = librosa.feature.mfcc(y=sample, sr=16000, hop_length=512, n_fft=2048).transpose().astype(np.float32)\n",
    "\n",
    "        label = self.label_list[idx]\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "    def get_classes(self):\n",
    "        return [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"enable\", \"stop\", \"start\", \"backward\", \"cancel\", \"close\", \"rotate\", \"digit\", \"direction\", \"disable\", \n",
    "        \"zoom in\",\"zoom out\",\"undo\",\"enter\",\"forward\",\"move\",\"next\",\"record\",\"receive\",\"previous\"]\n",
    "\n",
    "    def get_valid_filenames(self):\n",
    "        class_names = self.get_classes()\n",
    "\n",
    "        class_to_filename = defaultdict(set)\n",
    "        with open(self.root_dir + \"dataset/val.txt\", \"r\") as fp:\n",
    "            for line in fp:\n",
    "                clean_line = line.strip().split(\"/\")\n",
    "\n",
    "                if clean_line[0] in class_names:\n",
    "                    class_to_filename[clean_line[0]].add(clean_line[1])\n",
    "\n",
    "        return class_to_filename\n",
    "\n",
    "    def get_test_filenames(self):\n",
    "        class_names = self.get_classes()\n",
    "\n",
    "        class_to_filename = defaultdict(set)\n",
    "        with open(self.root_dir + \"dataset/test.txt\", \"r\") as fp:\n",
    "            for line in fp:\n",
    "                clean_line = line.strip().split(\"/\")\n",
    "\n",
    "                if clean_line[0] in class_names:\n",
    "                    class_to_filename[clean_line[0]].add(clean_line[1])\n",
    "\n",
    "        return class_to_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407567f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_audiomentations import Compose, Gain, PolarityInversion\n",
    "\n",
    "\n",
    "# Initialize augmentation callable\n",
    "train_transforms = Compose(\n",
    "    transforms=[\n",
    "        Gain(\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        PolarityInversion(p=0.5)\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    transforms=[\n",
    "        Gain(\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        PolarityInversion(p=0.5)\n",
    "    ]\n",
    ")\n",
    "test_transforms = Compose(\n",
    "    transforms=[\n",
    "        Gain(\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        PolarityInversion(p=0.5)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ea675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder=\"Arabic_Speech_command/\"\n",
    "\n",
    "train_dataset = SpeechCommandsDataset(dataset_folder,\n",
    "                                      \"train\",transform=train_transforms)\n",
    "valid_dataset = SpeechCommandsDataset(dataset_folder,\n",
    "                                      \"valid\",transform=val_transforms)\n",
    "\n",
    "test_dataset = SpeechCommandsDataset(dataset_folder,\n",
    "                                     \"test\",transform=test_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce6a8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "valid_every_n_steps = 20\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bcb18a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4681 74\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c09a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c5c901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "653343b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModel(nn.Module):\n",
    "    \"\"\"Neural network model (Transformer-based).\n",
    "\n",
    "    Args:\n",
    "        idim (int): Input feature dimension.\n",
    "        d_att (int): Attention dimension.\n",
    "        n_heads (int): The number of attention heads.\n",
    "        d_ff (int): Dimension of feed forward network.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "        n_layers (int): The number of encoder layers.\n",
    "        d_linear (int): Dimension of a hidden layer of the classifier.\n",
    "        n_classes (int): The number of the output classes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        idim=13, \n",
    "        d_att=64, \n",
    "        n_heads=2, \n",
    "        d_ff=512, \n",
    "        dropout_rate=0.1, \n",
    "        n_layers=3, \n",
    "        n_classes=40\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.subsampling = Subsampling(idim=idim, d_att=d_att)\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        self.encoder_layers = nn.Sequential()\n",
    "        for i in range(n_layers):\n",
    "            self.encoder_layers.add_module(\n",
    "                f'EncoderLayer{i}', \n",
    "                TransformerEncoderLayer(d_att, n_heads, d_ff, dropout_rate)\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(d_att)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.out = nn.Linear(d_att, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Recognize the input speech commands.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input features (batch, tmax, idim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Recognized classes (batch, num_classes).\n",
    "\n",
    "        \"\"\"   \n",
    "        # Transformer encoder\n",
    "        x = self.subsampling(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.encoder_layers(x)\n",
    "        x = self.norm(x)\n",
    "        # Classifier\n",
    "        x = torch.mean(x, dim=1)  # (b, t, d_att) -> (b, d_att)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"A Transformer encoder layer.\n",
    "\n",
    "    Args:\n",
    "        d_att (int): Attention dimension.\n",
    "        d_head (int): The number of attention heads.\n",
    "        d_ff (int): Dimension of feed forward network.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_att, d_head, d_ff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_att, d_head, dropout_rate)\n",
    "        self.ff = FeedForward(d_att, d_ff, dropout_rate)\n",
    "        self.norm_mha = nn.LayerNorm(d_att)\n",
    "        self.norm_ff = nn.LayerNorm(d_att)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Pre-encoded inputs (batch, tmax, d_att).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded outputs (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        # Multi-Head Attention\n",
    "        res = x\n",
    "        x = self.norm_mha(x)\n",
    "        x = res + self.dropout(self.mha(x, x, x))\n",
    "\n",
    "        # Feed-Forward\n",
    "        res = x\n",
    "        x = self.norm_ff(x)\n",
    "        x = res + self.dropout(self.ff(x))\n",
    "\n",
    "        return x\n",
    "class Subsampling(nn.Module):\n",
    "    \"\"\"Convolutional Subsampling.\n",
    "\n",
    "    Args:\n",
    "        idim (int): Input feature dimension.\n",
    "        d_att (int): Attention dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idim, d_att):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, d_att, kernel_size=(3, 3), stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(d_att, d_att, kernel_size=(3, 3), stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear = nn.Linear(256, d_att)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input features (batch, tmax, idim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Subsampled features (batch, tmax', d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)  # (b, t, idim) -> (b, c=1, t, idim)\n",
    "        x = self.conv(x)\n",
    "        b, c, t, f = x.size()\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = x.transpose(1, 2).contiguous().view(b, t, c * f)  # (b, c, t, f) -> (b, t, c * t)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding.\n",
    "\n",
    "    Args:\n",
    "        idim (int): Input feature dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "           super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x: (torch.Tensor): Subsampled features (batch, tmax, d_att).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded features (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        _, tmax, d_att = x.size()\n",
    "        pos = torch.arange(0, tmax, dtype=torch.float32).unsqueeze(1)\n",
    "        pe = torch.zeros(1, tmax, d_att, dtype=torch.float32).to(x.device)\n",
    "        pe[:, :, 0::2] = torch.sin(pos / torch.pow(10000, torch.arange(0, d_att, 2) / d_att))\n",
    "        pe[:, :, 1::2] = torch.cos(pos / torch.pow(10000, torch.arange(0, d_att, 2) / d_att))\n",
    "        x = x + pe\n",
    "\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention.\n",
    "\n",
    "    Args:\n",
    "        d_att (int): Dimension of attention.\n",
    "        d_head (int): The number of attention heads.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_att, n_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.linear_q = nn.Linear(d_att, d_att)\n",
    "        self.linear_k = nn.Linear(d_att, d_att)\n",
    "        self.linear_v = nn.Linear(d_att, d_att)\n",
    "        self.linear_head = nn.Linear(d_att, d_att)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_comn = d_att // self.n_heads\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            q: (torch.Tensor): Query (batch, tmax, d_att).\n",
    "            k: (torch.Tensor): Key (batch, tmax, d_att).\n",
    "            v: (torch.Tensor): Value (batch, tmax, d_att).\n",
    "            Returns:\n",
    "            torch.Tensor: Output shape (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        # Linear\n",
    "        qw = self.linear_q(q)\n",
    "        kw = self.linear_k(k)\n",
    "        vw = self.linear_v(v)\n",
    "\n",
    "        # Reshape tensor (b, t, d_att) -> (b, n_heads, t, d_comn)\n",
    "        b, t, d_att = q.size()\n",
    "        qw = qw.view(b, t, self.n_heads, self.d_comn).transpose(1, 2)\n",
    "        kw = kw.view(b, t, self.n_heads, self.d_comn).transpose(1, 2)\n",
    "        vw = vw.view(b, t, self.n_heads, self.d_comn).transpose(1, 2)\n",
    "\n",
    "        # Dot-attention\n",
    "        matmul = torch.matmul(qw, kw.transpose(2, 3)) \n",
    "        scale = matmul / torch.sqrt(torch.tensor(self.d_comn))\n",
    "        softmax = torch.softmax(scale, dim=-1)\n",
    "        att = torch.matmul(self.dropout(softmax), vw)  # (b, n_heads, t, d_comn)\n",
    "\n",
    "        # Concatenate\n",
    "        att = att.transpose(1, 2).contiguous().view(b, -1, self.n_heads * self.d_comn)  # (b, t, d_att)\n",
    "\n",
    "        # Linear\n",
    "        mha = self.linear_head(att)\n",
    "\n",
    "        return mha\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-Forward Network.\n",
    "\n",
    "    Args:\n",
    "        d_ff (int): Dimension of feed-forward network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_att, d_ff, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_att, d_ff), nn.ReLU(), nn.Dropout(dropout_rate), nn.Linear(d_ff, d_att)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x: (torch.Tensor): Input shape (batch, tmax, d_att).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output shape (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65a34c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# model parameters: 305,768\n",
      "Epoch: 1/50 - train_loss: 3.1678 - train_acc: 0.0961 - dev_loss: 2.7084 - dev_acc: 0.2431\n",
      "Epoch: 2/50 - train_loss: 2.1189 - train_acc: 0.4110 - dev_loss: 1.7671 - dev_acc: 0.4971\n",
      "Epoch: 3/50 - train_loss: 1.2914 - train_acc: 0.6492 - dev_loss: 1.3484 - dev_acc: 0.5683\n",
      "Epoch: 4/50 - train_loss: 0.8186 - train_acc: 0.7755 - dev_loss: 1.2435 - dev_acc: 0.5908\n",
      "Epoch: 5/50 - train_loss: 0.5651 - train_acc: 0.8400 - dev_loss: 0.9562 - dev_acc: 0.6902\n",
      "Epoch: 6/50 - train_loss: 0.3973 - train_acc: 0.8898 - dev_loss: 0.8948 - dev_acc: 0.7081\n",
      "Epoch: 7/50 - train_loss: 0.2839 - train_acc: 0.9272 - dev_loss: 0.7765 - dev_acc: 0.7441\n",
      "Epoch: 8/50 - train_loss: 0.2162 - train_acc: 0.9402 - dev_loss: 0.7458 - dev_acc: 0.7505\n",
      "Epoch: 9/50 - train_loss: 0.1839 - train_acc: 0.9481 - dev_loss: 0.8533 - dev_acc: 0.7588\n",
      "Epoch: 10/50 - train_loss: 0.1720 - train_acc: 0.9511 - dev_loss: 0.7891 - dev_acc: 0.7640\n",
      "Epoch: 11/50 - train_loss: 0.1207 - train_acc: 0.9701 - dev_loss: 0.7737 - dev_acc: 0.7710\n",
      "Epoch: 12/50 - train_loss: 0.0870 - train_acc: 0.9799 - dev_loss: 0.8288 - dev_acc: 0.7620\n",
      "Epoch: 13/50 - train_loss: 0.0717 - train_acc: 0.9829 - dev_loss: 0.7936 - dev_acc: 0.7710\n",
      "Epoch: 14/50 - train_loss: 0.0558 - train_acc: 0.9902 - dev_loss: 0.9743 - dev_acc: 0.7498\n",
      "Epoch: 15/50 - train_loss: 0.1493 - train_acc: 0.9541 - dev_loss: 0.7974 - dev_acc: 0.7774\n",
      "Epoch: 16/50 - train_loss: 0.0543 - train_acc: 0.9889 - dev_loss: 0.7474 - dev_acc: 0.7954\n",
      "Epoch: 17/50 - train_loss: 0.0379 - train_acc: 0.9919 - dev_loss: 0.7017 - dev_acc: 0.8037\n",
      "Epoch: 18/50 - train_loss: 0.0287 - train_acc: 0.9968 - dev_loss: 0.8023 - dev_acc: 0.7896\n",
      "Epoch: 19/50 - train_loss: 0.0194 - train_acc: 0.9966 - dev_loss: 0.7503 - dev_acc: 0.8133\n",
      "Epoch: 20/50 - train_loss: 0.0135 - train_acc: 0.9987 - dev_loss: 0.6908 - dev_acc: 0.8217\n",
      "Epoch: 21/50 - train_loss: 0.0076 - train_acc: 1.0000 - dev_loss: 0.8171 - dev_acc: 0.8031\n",
      "Epoch: 22/50 - train_loss: 0.0124 - train_acc: 0.9983 - dev_loss: 0.7400 - dev_acc: 0.8165\n",
      "Epoch: 23/50 - train_loss: 0.0114 - train_acc: 0.9991 - dev_loss: 0.7700 - dev_acc: 0.8133\n",
      "Epoch: 24/50 - train_loss: 0.0057 - train_acc: 1.0000 - dev_loss: 0.7843 - dev_acc: 0.8133\n",
      "Epoch: 25/50 - train_loss: 0.0039 - train_acc: 1.0000 - dev_loss: 0.8084 - dev_acc: 0.8069\n",
      "Epoch: 26/50 - train_loss: 0.0033 - train_acc: 1.0000 - dev_loss: 0.8023 - dev_acc: 0.8095\n",
      "Epoch: 27/50 - train_loss: 0.0025 - train_acc: 1.0000 - dev_loss: 0.8331 - dev_acc: 0.8133\n",
      "Epoch: 28/50 - train_loss: 0.0022 - train_acc: 1.0000 - dev_loss: 0.7967 - dev_acc: 0.8185\n",
      "Epoch: 29/50 - train_loss: 0.0019 - train_acc: 1.0000 - dev_loss: 0.7998 - dev_acc: 0.8198\n",
      "Epoch: 30/50 - train_loss: 0.0016 - train_acc: 1.0000 - dev_loss: 0.8185 - dev_acc: 0.8153\n",
      "Epoch: 31/50 - train_loss: 0.0015 - train_acc: 1.0000 - dev_loss: 0.8133 - dev_acc: 0.8210\n",
      "Epoch: 32/50 - train_loss: 0.0016 - train_acc: 1.0000 - dev_loss: 0.8374 - dev_acc: 0.8146\n",
      "Epoch: 33/50 - train_loss: 0.0015 - train_acc: 1.0000 - dev_loss: 0.8330 - dev_acc: 0.8146\n",
      "Epoch: 34/50 - train_loss: 0.0014 - train_acc: 1.0000 - dev_loss: 0.8112 - dev_acc: 0.8204\n",
      "Epoch: 35/50 - train_loss: 0.0012 - train_acc: 1.0000 - dev_loss: 0.8524 - dev_acc: 0.8172\n",
      "Epoch: 36/50 - train_loss: 0.0012 - train_acc: 1.0000 - dev_loss: 0.8954 - dev_acc: 0.8191\n",
      "Epoch: 37/50 - train_loss: 0.0011 - train_acc: 1.0000 - dev_loss: 0.8685 - dev_acc: 0.8127\n",
      "Epoch: 38/50 - train_loss: 0.0012 - train_acc: 1.0000 - dev_loss: 0.9923 - dev_acc: 0.8044\n",
      "Epoch: 39/50 - train_loss: 0.2514 - train_acc: 0.9169 - dev_loss: 1.0522 - dev_acc: 0.7357\n",
      "Epoch: 40/50 - train_loss: 0.1516 - train_acc: 0.9558 - dev_loss: 0.7581 - dev_acc: 0.7826\n",
      "Epoch: 41/50 - train_loss: 0.0777 - train_acc: 0.9776 - dev_loss: 1.0472 - dev_acc: 0.7543\n",
      "Epoch: 42/50 - train_loss: 0.0390 - train_acc: 0.9906 - dev_loss: 0.7573 - dev_acc: 0.8089\n",
      "Epoch: 43/50 - train_loss: 0.0203 - train_acc: 0.9964 - dev_loss: 0.7873 - dev_acc: 0.8127\n",
      "Epoch: 44/50 - train_loss: 0.0114 - train_acc: 0.9981 - dev_loss: 0.7656 - dev_acc: 0.8242\n",
      "Epoch: 45/50 - train_loss: 0.0179 - train_acc: 0.9959 - dev_loss: 0.7834 - dev_acc: 0.8044\n",
      "Epoch: 46/50 - train_loss: 0.0114 - train_acc: 0.9974 - dev_loss: 0.7271 - dev_acc: 0.8210\n",
      "Epoch: 47/50 - train_loss: 0.0059 - train_acc: 0.9996 - dev_loss: 0.7650 - dev_acc: 0.8275\n",
      "Epoch: 48/50 - train_loss: 0.0031 - train_acc: 1.0000 - dev_loss: 0.7403 - dev_acc: 0.8307\n",
      "Epoch: 49/50 - train_loss: 0.0020 - train_acc: 1.0000 - dev_loss: 0.7773 - dev_acc: 0.8300\n",
      "Epoch: 50/50 - train_loss: 0.0016 - train_acc: 1.0000 - dev_loss: 0.7893 - dev_acc: 0.8268\n"
     ]
    }
   ],
   "source": [
    "# Build a model\n",
    "model = NeuralNetworkModel().to(device)\n",
    "print(f\"# model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Define an optimizer and a loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "#optimizer = optim.SGD(model.parameters(), lr= 0.1, weight_decay=1e-5, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "dev_acc_list = []\n",
    "dev_loss_list = []\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "# Make a directory to save the results\n",
    "os.makedirs(\"result\", exist_ok=True)\n",
    "\n",
    "# Train the model\n",
    "training_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    batch_train_loss = 0\n",
    "    batch_train_acc = 0\n",
    "    batch_dev_loss = 0\n",
    "    batch_dev_acc = 0\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for feats, labels in train_loader:\n",
    "        feats, labels = feats.to(device), labels.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(feats)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_train_loss += loss.item()\n",
    "        batch_train_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "\n",
    "    train_loss_list.append(batch_train_loss / len(train_loader))\n",
    "    train_acc_list.append(batch_train_acc / len(train_loader.dataset))\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for feats, labels in valid_loader:\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_dev_loss += loss.item()\n",
    "            batch_dev_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "\n",
    "    dev_loss_list.append(batch_dev_loss / len(valid_loader))\n",
    "    dev_acc_list.append(batch_dev_acc / len(valid_loader.dataset))\n",
    "\n",
    "    # Save the best model\n",
    "    if best_loss > dev_loss_list[-1]:\n",
    "        torch.save(model.state_dict(), \"result/model_best_loss.pth\")\n",
    "        best_loss = dev_loss_list[-1]\n",
    "\n",
    "    # Print statistics\n",
    "    print(\n",
    "        \"Epoch: {}/{} - train_loss: {:.4f} - train_acc: {:.4f} - dev_loss: {:.4f} - dev_acc: {:.4f}\".format(\n",
    "            epoch+1, epochs, train_loss_list[-1], train_acc_list[-1], dev_loss_list[-1], dev_acc_list[-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Save the training time\n",
    "training_time = time.time() - training_start_time\n",
    "with open(\"result/training_time.txt\", \"w\") as time_f:\n",
    "    time_f.write(f\"training time = {int(training_time)} (sec)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa840b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
