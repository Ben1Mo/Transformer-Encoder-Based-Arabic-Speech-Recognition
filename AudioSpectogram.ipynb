{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "\n",
    "import pickle5 as pickle\n",
    "import tqdm\n",
    "import IPython.display\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tabulate import tabulate\n",
    "from IPython.display import Audio\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.io.wavfile import read\n",
    "\n",
    "from linformer_pytorch import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from functools import partial\n",
    "\n",
    "from ray import tune, air\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "torch.use_deterministic_algorithms(mode=True)\n",
    "\n",
    "random_seed = 17 # or any of your favorite number\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.random.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 24 11:52:31 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   43C    P8    N/A /  N/A |    253MiB /  4042MiB |      1%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      3154      G   /usr/lib/xorg/Xorg                 28MiB |\r\n",
      "|    0   N/A  N/A      4176      G   /usr/bin/gnome-shell               68MiB |\r\n",
      "|    0   N/A  N/A      4753      G   /usr/lib/xorg/Xorg                 98MiB |\r\n",
      "|    0   N/A  N/A      4915      G   /usr/bin/gnome-shell               55MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandsDataset(Dataset):\n",
    "    \"\"\"Google Speech Commands dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the data files.\n",
    "            split    (string): In [\"train\", \"valid\", \"test\"].\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.number_of_classes = len(self.get_classes())\n",
    "\n",
    "        self.class_to_file = defaultdict(list)\n",
    "\n",
    "        self.valid_filenames = self.get_valid_filenames()\n",
    "        self.test_filenames = self.get_test_filenames()\n",
    "\n",
    "        for c in self.get_classes():\n",
    "            file_name_list = sorted(os.listdir(self.root_dir + \"dataset/\" + c))\n",
    "            for filename in file_name_list:\n",
    "                if split == \"train\":\n",
    "                    if (filename not in self.valid_filenames[c]) and (filename not in self.test_filenames[c]):\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                elif split == \"valid\":\n",
    "                    if filename in self.valid_filenames[c]:\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                elif split == \"test\":\n",
    "                    if filename in self.test_filenames[c]:\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid split name.\")\n",
    "\n",
    "        self.filepath_list = list()\n",
    "        self.label_list = list()\n",
    "        for cc, c in enumerate(self.get_classes()):\n",
    "            f_extension = sorted(list(self.class_to_file[c]))\n",
    "            l_extension = [cc for i in f_extension]\n",
    "            f_extension = [self.root_dir + \"dataset/\" + c + \"/\" + filename for filename in f_extension]\n",
    "            self.filepath_list.extend(f_extension)\n",
    "            self.label_list.extend(l_extension)\n",
    "        self.number_of_samples = len(self.filepath_list)\n",
    "    def __len__(self):\n",
    "        return self.number_of_samples\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        sample = np.zeros((16000, ), dtype=np.float32)\n",
    "\n",
    "        sample_file = self.filepath_list[idx]\n",
    "\n",
    "        sample_from_file = read(sample_file)[1]\n",
    "\n",
    "        sample[:sample_from_file.size] = sample_from_file\n",
    "        sample = sample.reshape((16000, ))\n",
    "\n",
    "        # It is better to stick to PyTorch functions\n",
    "        # I swapped the `librosaÂ´ mfcc resampling with this PyTorch alternative\n",
    "        # however, the parameters are the same\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        mfcc_transform = T.MFCC(\n",
    "            sample_rate=16000,\n",
    "            n_mfcc=20,\n",
    "            melkwargs={\n",
    "              'n_fft': 2048,\n",
    "              'hop_length': 512})\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        sample = torch.transpose(mfcc_transform(torch.tensor(sample)), 0, 1)\n",
    "\n",
    "        #sample = librosa.feature.mfcc(y=sample, sr=16000, hop_length=512, n_fft=2048).transpose().astype(np.float32)\n",
    "\n",
    "        # we apply the augmentations on the sample if it is defined\n",
    "        if self.transform:\n",
    "            torch.manual_seed(random_seed)\n",
    "            torch.cuda.manual_seed(random_seed)\n",
    "            np.random.seed(random_seed)\n",
    "            random.seed(random_seed)\n",
    "            sample = self.transform(samples=sample.reshape(32, 1, 20), sample_rate=16000)\n",
    "            sample = sample.reshape(32, 20)\n",
    "\n",
    "        label = self.label_list[idx]\n",
    "\n",
    "        return sample, label\n",
    "    \n",
    "\n",
    "    def get_classes(self):\n",
    "        return [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\",\"right\", \n",
    "                \"left\", \"up\", \"down\", \"forward\", \"backward\", \"yes\", \"no\", \"stop\", \"start\", \"enable\", \n",
    "                \"disable\", \"ok\", \"cancel\", \"open\", \"close\", \"zoom in\", \"zoom out\", \"previous\", \"next\", \n",
    "                \"send\", \"receive\", \"move\", \"rotate\", \"record\", \"enter\", \"digit\", \"direction\", \"options\", \"undo\"]\n",
    "\n",
    "    def get_valid_filenames(self):\n",
    "        class_names = self.get_classes()\n",
    "\n",
    "        class_to_filename = defaultdict(set)\n",
    "        with open(self.root_dir + \"dataset/val.txt\", \"r\") as fp:\n",
    "            for line in fp:\n",
    "                clean_line = line.strip().split(\"/\")\n",
    "\n",
    "                if clean_line[0] in class_names:\n",
    "                    class_to_filename[clean_line[0]].add(clean_line[1])\n",
    "\n",
    "        return class_to_filename\n",
    "\n",
    "    def get_test_filenames(self):\n",
    "        class_names = self.get_classes()\n",
    "\n",
    "        class_to_filename = defaultdict(set)\n",
    "        with open(self.root_dir + \"dataset/test.txt\", \"r\") as fp:\n",
    "            for line in fp:\n",
    "                clean_line = line.strip().split(\"/\")\n",
    "\n",
    "                if clean_line[0] in class_names:\n",
    "                    class_to_filename[clean_line[0]].add(clean_line[1])\n",
    "\n",
    "        return class_to_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import Compose, Gain, PolarityInversion\n",
    "\n",
    "# Initialize augmentation callable\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "train_transforms = Compose(\n",
    "    transforms=[\n",
    "        Gain(\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        PolarityInversion(p=0.5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "val_transforms = Compose(\n",
    "    transforms=[\n",
    "        Gain(\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        PolarityInversion(p=0.5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "test_transforms = Compose(\n",
    "    transforms=[\n",
    "        Gain(\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        PolarityInversion(p=0.5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder= os.path.join(os.getcwd(), \"Arabic_Speech_command/\")\n",
    "\n",
    "train_dataset = SpeechCommandsDataset(dataset_folder, \"train\")\n",
    "\n",
    "valid_dataset = SpeechCommandsDataset(dataset_folder, \"valid\")\n",
    "\n",
    "test_dataset  = SpeechCommandsDataset(dataset_folder, \"test\")\n",
    "\n",
    "### Augmented Datasets ###\n",
    "# Note we should concatenate both the non-augmented and the augmented training datasets\n",
    "\n",
    "train_dataset_aug = ConcatDataset([SpeechCommandsDataset(dataset_folder, \"train\", \n",
    "                                                         transform=train_transforms),\n",
    "                                                         train_dataset])\n",
    "\n",
    "valid_dataset_aug = SpeechCommandsDataset(dataset_folder, \"valid\", transform=val_transforms)\n",
    "\n",
    "test_dataset_aug  = SpeechCommandsDataset(dataset_folder, \"test\", transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset, 7201 samples.\n",
      "Validation dataset, 2399 samples.\n",
      "Testing dataset, 2400 samples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset, {len(train_dataset)} samples.\")\n",
    "\n",
    "print(f\"Validation dataset, {len(valid_dataset)} samples.\")\n",
    "\n",
    "print(f\"Testing dataset, {len(test_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModel(nn.Module):\n",
    "    \"\"\"Neural network model (Transformer-based).\n",
    "\n",
    "    Args:\n",
    "        idim (int): Input feature dimension.\n",
    "        d_att (int): Attention dimension.\n",
    "        n_heads (int): The number of attention heads.\n",
    "        d_ff (int): Dimension of feed forward network.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "        n_layers (int): The number of encoder layers.\n",
    "        d_linear (int): Dimension of a hidden layer of the classifier.\n",
    "        n_classes (int): The number of the output classes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        idim=13,\n",
    "        d_att=64,\n",
    "        n_heads=4,\n",
    "        d_ff=512,\n",
    "        dropout_rate=0.1,\n",
    "        n_layers=3,\n",
    "        n_classes=40\n",
    "    ):\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        super().__init__()\n",
    "        self.subsampling = Subsampling(idim=idim, d_att=d_att)\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.encoder_layers = nn.Sequential()\n",
    "        for i in range(n_layers):\n",
    "            self.encoder_layers.add_module(\n",
    "                f'EncoderLayer{i}', \n",
    "                TransformerEncoderLayer(d_att, n_heads, d_ff, dropout_rate)\n",
    "            )\n",
    "            \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.norm = nn.LayerNorm(d_att)#, elementwise_affine=False)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.out = nn.Linear(d_att, n_classes)#, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Recognize the input speech commands.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input features (batch, tmax, idim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Recognized classes (batch, num_classes).\n",
    "\n",
    "        \"\"\"   \n",
    "        # Transformer encoder\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = self.subsampling(x)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = self.positional_encoding(x)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = self.encoder_layers(x)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = self.norm(x)\n",
    "        # Classifier\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = torch.mean(x, dim=1)  # (b, t, d_att) -> (b, d_att)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = self.dropout(x)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"A Transformer encoder layer.\n",
    "\n",
    "    Args:\n",
    "        d_att (int): Attention dimension.\n",
    "        d_head (int): The number of attention heads.\n",
    "        d_ff (int): Dimension of feed forward network.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_att, d_head, d_ff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.mha = MultiHeadAttention(d_att, d_head, dropout_rate)\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.ff = FeedForward(d_att, d_ff, dropout_rate)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.norm_mha = nn.LayerNorm(d_att)#, elementwise_affine=False)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.norm_ff = nn.LayerNorm(d_att)#, elementwise_affine=False)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Pre-encoded inputs (batch, tmax, d_att).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded outputs (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        # Multi-Head Attention\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        res = x\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = self.norm_mha(x)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = res + self.dropout(self.mha(x, x, x))\n",
    "\n",
    "        # Feed-Forward\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        res = x\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = self.norm_ff(x)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = res + self.dropout(self.ff(x))\n",
    "\n",
    "        return x\n",
    "class Subsampling(nn.Module):\n",
    "    \"\"\"Convolutional Subsampling.\n",
    "\n",
    "    Args:\n",
    "        idim (int): Input feature dimension.\n",
    "        d_att (int): Attention dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idim, d_att):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, d_att, kernel_size=(3, 3), stride=(2, 2)),#, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(d_att, d_att, kernel_size=(3, 3), stride=(2, 2)),#, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.linear = nn.Linear(256, d_att)#, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input features (batch, tmax, idim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Subsampled features (batch, tmax', d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = x.unsqueeze(1)  # (b, t, idim) -> (b, c=1, t, idim)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = self.conv(x)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        b, c, t, f = x.size()\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = x.transpose(1, 2).contiguous().view(b, t, c * f)  # (b, c, t, f) -> (b, t, c * t)\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding.\n",
    "\n",
    "    Args:\n",
    "        idim (int): Input feature dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "           super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x: (torch.Tensor): Subsampled features (batch, tmax, d_att).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded features (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        _, tmax, d_att = x.size()\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        pos = torch.arange(0, tmax, dtype=torch.float32).unsqueeze(1)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        pe = torch.zeros(1, tmax, d_att, dtype=torch.float32).to(x.device)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        pe[:, :, 0::2] = torch.sin(pos / torch.pow(10000, torch.arange(0, d_att, 2) / d_att))\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        pe[:, :, 1::2] = torch.cos(pos / torch.pow(10000, torch.arange(0, d_att, 2) / d_att))\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        x = x + pe\n",
    "\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention.\n",
    "\n",
    "    Args:\n",
    "        d_att (int): Dimension of attention.\n",
    "        d_head (int): The number of attention heads.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_att, n_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.linear_q = nn.Linear(d_att, d_att)#, bias=False)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.linear_k = nn.Linear(d_att, d_att)#, bias=False)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.linear_v = nn.Linear(d_att, d_att)#, bias=False)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.linear_head = nn.Linear(d_att, d_att)#, bias=False)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_comn = d_att // self.n_heads\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            q: (torch.Tensor): Query (batch, tmax, d_att).\n",
    "            k: (torch.Tensor): Key (batch, tmax, d_att).\n",
    "            v: (torch.Tensor): Value (batch, tmax, d_att).\n",
    "            Returns:\n",
    "            torch.Tensor: Output shape (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        # Linear\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        qw = self.linear_q(q)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        kw = self.linear_k(k)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        vw = self.linear_v(v)\n",
    "\n",
    "        # Reshape tensor (b, t, d_att) -> (b, n_heads, t, d_comn)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        b, t, d_att = q.size()\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        qw = qw.view(b, t, self.n_heads, self.d_comn).transpose(1, 2)\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        kw = kw.view(b, t, self.n_heads, self.d_comn).transpose(1, 2)\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        vw = vw.view(b, t, self.n_heads, self.d_comn).transpose(1, 2)\n",
    "\n",
    "        # Dot-attention\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        matmul = torch.matmul(qw, kw.transpose(2, 3))\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        scale = matmul / torch.sqrt(torch.tensor(self.d_comn))\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        softmax = torch.softmax(scale, dim=-1)\n",
    "        \n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        att = torch.matmul(self.dropout(softmax), vw)  # (b, n_heads, t, d_comn)\n",
    "\n",
    "        # Concatenate\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        att = att.transpose(1, 2).contiguous().view(b, -1, self.n_heads * self.d_comn)  # (b, t, d_att)\n",
    "\n",
    "        # Linear\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        mha = self.linear_head(att)\n",
    "\n",
    "        return mha\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-Forward Network.\n",
    "\n",
    "    Args:\n",
    "        d_ff (int): Dimension of feed-forward network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_att, d_ff, dropout_rate):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_att, d_ff),#, bias=False), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout_rate), \n",
    "            nn.Linear(d_ff, d_att)#, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x: (torch.Tensor): Input shape (batch, tmax, d_att).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output shape (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd81bc7f138>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some metric lists\n",
    "def train(config, \n",
    "          train_set, \n",
    "          val_set, \n",
    "          results_dir='./results',\n",
    "          continue_training=False,\n",
    "          epochs=5, \n",
    "          plot=False):\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    model = NeuralNetworkModel(n_heads=config[\"n_heads\"], \n",
    "                               n_layers=config[\"n_layers\"]\n",
    "                               ).to(device)\n",
    "    \n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    \n",
    "    if continue_training:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(results_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "        \n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        worker_init_fn=seed_worker)\n",
    "    \n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    valid_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        worker_init_fn=seed_worker)\n",
    "    \n",
    "\n",
    "    #store evaluation metrics in these lists\n",
    "    train_acc_list = []\n",
    "    train_loss_list = []\n",
    "    dev_acc_list = []\n",
    "    dev_loss_list = []\n",
    "    \n",
    "    # best loss tracking for model checkpointing\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        batch_train_loss = 0\n",
    "        batch_train_acc = 0\n",
    "        batch_dev_loss = 0\n",
    "        batch_dev_acc = 0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for feats, labels in train_loader:\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_train_loss += loss.item()\n",
    "            batch_train_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "\n",
    "        train_loss_list.append(batch_train_loss / len(train_loader))\n",
    "        train_acc_list.append(batch_train_acc / len(train_loader.dataset))\n",
    "        # Validation\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for feats, labels in valid_loader:\n",
    "                feats, labels = feats.to(device), labels.to(device)\n",
    "\n",
    "                # Forward\n",
    "                outputs = model(feats)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                batch_dev_loss += loss.item()\n",
    "                batch_dev_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "\n",
    "        dev_loss_list.append(batch_dev_loss / len(valid_loader))\n",
    "        dev_acc_list.append(batch_dev_acc / len(valid_loader.dataset))\n",
    "        \n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        torch.save((model.state_dict(), optimizer.state_dict()), os.path.join(results_dir, f\"checkpoint{epoch}.pt\"))\n",
    "        \n",
    "        checkpoint = Checkpoint.from_directory(results_dir)\n",
    "        session.report({\n",
    "                        \"loss\" : dev_loss_list[-1],\n",
    "                        \"accuracy\" : dev_acc_list[-1]\n",
    "                        },\n",
    "                        checkpoint=checkpoint)\n",
    "        \n",
    "        # plot results if flag is True\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 6.5))\n",
    "\n",
    "            ax[0].plot(range(len(train_loss_list)), train_loss_list, label='train loss')\n",
    "            ax[0].plot(range(len(dev_loss_list)), dev_loss_list, label='val loss')\n",
    "            ax[0].set_title('Loss Evolution')\n",
    "\n",
    "            ax[1].plot(range(len(train_acc_list)), train_acc_list, label='train accuracy')\n",
    "            ax[1].plot(range(len(dev_acc_list)), dev_acc_list, label='val accuracy')\n",
    "            ax[1].set_title('Accuracy Evolution')\n",
    "\n",
    "            ax[0].legend()\n",
    "            ax[1].legend()\n",
    "\n",
    "            plt.show()\n",
    "        \n",
    "def test(model, test_set, device, batch_size=64):\n",
    "    best_loss = float(\"inf\")\n",
    "    \n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "    # Test\n",
    "    test_start_time = time.time()\n",
    "    \n",
    "    batch_test_loss = 0\n",
    "    batch_test_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feats, labels in test_loader:\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(feats)\n",
    "            batch_test_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "\n",
    "    test_mean_acc = batch_test_acc / len(test_loader.dataset)\n",
    "\n",
    "    # Print out test time\n",
    "    test_time = time.time() - test_start_time\n",
    "    print(f\">> Test Time: {test_time}\")\n",
    "    \n",
    "    return test_mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validation Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-06-24 11:57:31</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:58.15        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.4/7.6 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: 0.33722384326802834 | Iter 4.000: 0.19716548561900793 | Iter 2.000: 0.11963318049187162 | Iter 1.000: 0.04585243851604835<br>Resources requested: 8.0/12 CPUs, 1.0/1 GPUs, 0.0/2.68 GiB heap, 0.0/1.34 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  n_heads</th><th style=\"text-align: right;\">  n_layers</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_with_parameters_fb485_00000</td><td>RUNNING </td><td>139.174.120.39:13436</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">3.39302e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        11</td><td style=\"text-align: right;\">2.0663</td><td style=\"text-align: right;\">  0.500208</td><td style=\"text-align: right;\">                  13</td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00001</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">2.57172e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00002</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">1.09411e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        11</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00003</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">3.71892e-05</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00004</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">4.49178e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        11</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00005</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">2.38701e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00006</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">2.48913e-05</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00007</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">1.98042e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00008</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">1.96028e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00009</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">4.10786e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00010</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">4.75075e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00011</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">6.55373e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00012</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">4.56253e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00013</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">1.63342e-05</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">        11</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00014</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">6.75002e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00015</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">8.95918e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>tune_with_parameters_fb485_00016</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">9.64681e-05</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 11:53:31,074\tINFO worker.py:1550 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th style=\"text-align: right;\">  accuracy</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  loss</th><th>node_ip       </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_with_parameters_fb485_00000</td><td style=\"text-align: right;\">  0.522718</td><td>2023-06-24_11-57-34</td><td>False </td><td>                </td><td>1faf94f2d19147c189496039a19d5ef4</td><td>mbe19     </td><td style=\"text-align: right;\">                        14</td><td style=\"text-align: right;\">1.9866</td><td>139.174.120.39</td><td style=\"text-align: right;\">13436</td><td>True               </td><td style=\"text-align: right;\">             238.094</td><td style=\"text-align: right;\">           17.7701</td><td style=\"text-align: right;\">       238.094</td><td style=\"text-align: right;\"> 1687600654</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  14</td><td>fb485_00000</td><td style=\"text-align: right;\">   0.00302267</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_samples=500\n",
    "max_num_epochs=100\n",
    "\n",
    "config = {\n",
    "    \"n_heads\": tune.choice([8, 16]),\n",
    "    \"n_layers\": tune.choice([10, 11, 12, 13]),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-4),\n",
    "    \"batch_size\": tune.choice([16, 32, 64, 128])\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"accuracy\",\n",
    "        mode=\"max\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(\n",
    "                    partial(train, \n",
    "                        train_set=train_dataset, \n",
    "                        val_set=valid_dataset,\n",
    "                        epochs=max_num_epochs)),\n",
    "            resources={\"cpu\": 8, \"gpu\": 1}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=air.RunConfig(local_dir=\"/media/mohamed/FCF05CB8F05C7B3A/ray_results\",\n",
    "                                 progress_reporter=reporter),\n",
    "    )\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "best_trial = results.get_best_result(\"accuracy\", \"max\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-05-08 13:28:34</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:25.58        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.5/7.6 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 4.000: 0.06419341392246769 | Iter 2.000: 0.02042517715714881 | Iter 1.000: 0.020842017507294707<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/2.89 GiB heap, 0.0/1.44 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_with_parameters_44d8d_00000</td><td>TERMINATED</td><td>139.174.120.27:7683</td><td style=\"text-align: right;\">3.58259</td><td style=\"text-align: right;\"> 0.0854523</td><td style=\"text-align: right;\">                   5</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th style=\"text-align: right;\">  accuracy</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">   loss</th><th>node_ip       </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_with_parameters_44d8d_00000</td><td style=\"text-align: right;\"> 0.0854523</td><td>2023-05-08_13-28-34</td><td>True  </td><td>                </td><td>bee6f4a09a254e29ac2b80958fad9e93</td><td>mbe19     </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">3.58259</td><td>139.174.120.27</td><td style=\"text-align: right;\"> 7683</td><td>True               </td><td style=\"text-align: right;\">             82.9564</td><td style=\"text-align: right;\">           16.3727</td><td style=\"text-align: right;\">       82.9564</td><td style=\"text-align: right;\"> 1683545314</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>44d8d_00000</td><td style=\"text-align: right;\">    0.0028758</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 13:28:34,530\tINFO tune.py:799 -- Total run time: 85.63 seconds (85.57 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'n_heads': 4, 'n_layers': 8, 'lr': 1e-05, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "num_samples=1\n",
    "max_num_epochs=5\n",
    "\n",
    "config = {\n",
    "    \"n_heads\": 4,#tune.choice([4, 8, 16]),\n",
    "    \"n_layers\": 8,#tune.choice([9, 10, 11, 12, 13, 14]),\n",
    "    \"lr\": 1e-5,#tune.loguniform(1e-5, 1e-4),\n",
    "    \"batch_size\": 64#tune.choice([32, 64, 128])\n",
    "}\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"accuracy\",\n",
    "        mode=\"max\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(\n",
    "                    partial(train, \n",
    "                        train_set=train_dataset, \n",
    "                        val_set=valid_dataset,\n",
    "                        epochs=max_num_epochs)),\n",
    "            resources={\"cpu\": 8, \"gpu\": 1}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=air.RunConfig(local_dir=\"/media/mohamed/FCF05CB8F05C7B3A/ray_results\",\n",
    "                                 progress_reporter=reporter),\n",
    "    )\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "results = tuner.fit()\n",
    "\n",
    "best_trial = results.get_best_result(\"accuracy\", \"max\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(metrics={'loss': 1.0552736018833362, 'accuracy': 0.8315964985410588, 'should_checkpoint': True, 'done': True, 'trial_id': '6a021_00350', 'experiment_tag': '350_batch_size=64,dropout_rate=0.1000,lr=0.0008,n_heads=16,n_layers=5'}, error=None, log_dir=PosixPath('/media/mohamed/FCF05CB8F05C7B3A/ray_results/tune_with_parameters_2023-03-26_13-28-59/tune_with_parameters_6a021_00350_350_batch_size=64,dropout_rate=0.1000,lr=0.0008,n_heads=16,n_layers=5_2023-03-26_18-06-36'))\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "\n",
    "best_trained_model = NeuralNetworkModel(dropout_rate=best_trial.config[\"dropout_rate\"], \n",
    "                                        n_heads=best_trial.config[\"n_heads\"])\n",
    "\n",
    "best_trained_model.to(device)\n",
    "\n",
    "best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric=\"loss\", mode=\"min\")\n",
    "best_checkpoint_dir = best_checkpoint.to_directory(path=\"results\")\n",
    "\n",
    "model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint.pt\"))\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "test_acc = test(model=best_trained_model, test_set=test_dataset,\n",
    "                device=device, batch_size=best_trial.config[\"batch_size\"])\n",
    "\n",
    "print(f\"Best trial test set accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Data (augmented) with Validation Data (Augmented)\n",
    "# We can notice that the max accuracy is reaching a value of ~ 77%\n",
    "\n",
    "# Build a model\n",
    "model = NeuralNetworkModel(dropout_rate=best_trial.config[\"dropout_rate\"], \n",
    "                               n_heads=best_trial.config[\"n_heads\"]).to(device)\n",
    "\n",
    "#best_trial.config[\"lr\"]\n",
    "#best_trial.config[\"batch_size\"]\n",
    "        \n",
    "print(f\"# model parameters: {sum(p.numel() for p in aug_model.parameters()):,}\")\n",
    "\n",
    "# Define an optimizer and a loss function\n",
    "optimizer = optim.Adam(aug_model.parameters(), lr=best_trial.config[\"lr\"])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train(, optimizer=optimizer, \n",
    "      train_loader=train_loader_aug, valid_loader=valid_loader_aug, \n",
    "      criterion=aug_loss_fn, epochs=epochs, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the model with the test augmented data\n",
    "\n",
    "test(model=aug_model, test_loader=test_loader_aug, criterion=aug_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the model with the test augmented data\n",
    "\n",
    "test(model=baseline_model, test_loader=test_loader_aug, criterion=baseline_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the model with the test non-augmented data\n",
    "\n",
    "test(model=aug_model, test_loader=test_loader, criterion=aug_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the model with the test non-augmented data\n",
    "\n",
    "test(model=baseline_model, test_loader=test_loader, criterion=baseline_loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
