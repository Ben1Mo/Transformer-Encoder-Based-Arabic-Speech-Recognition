{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfd5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "\n",
    "import pickle5 as pickle\n",
    "import tqdm\n",
    "import IPython.display\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tabulate import tabulate\n",
    "from IPython.display import Audio\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.io.wavfile import read\n",
    "\n",
    "from linformer_pytorch import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from functools import partial\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8250916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 17 # or any of your favorite number\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e49853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 25 10:22:17 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   37C    P8    N/A /  N/A |    240MiB /  4042MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      4058      G   /usr/lib/xorg/Xorg                 28MiB |\r\n",
      "|    0   N/A  N/A      4519      G   /usr/bin/gnome-shell               67MiB |\r\n",
      "|    0   N/A  N/A      5140      G   /usr/lib/xorg/Xorg                 84MiB |\r\n",
      "|    0   N/A  N/A      5301      G   /usr/bin/gnome-shell               57MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bfd749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca710a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandsDataset(Dataset):\n",
    "    \"\"\"Google Speech Commands dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the data files.\n",
    "            split    (string): In [\"train\", \"valid\", \"test\"].\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.number_of_classes = len(self.get_classes())\n",
    "\n",
    "        self.class_to_file = defaultdict(list)\n",
    "\n",
    "        self.valid_filenames = self.get_valid_filenames()\n",
    "        self.test_filenames = self.get_test_filenames()\n",
    "\n",
    "        for c in self.get_classes():\n",
    "            file_name_list = sorted(os.listdir(self.root_dir + \"dataset/\" + c))\n",
    "            for filename in file_name_list:\n",
    "                if split == \"train\":\n",
    "                    if (filename not in self.valid_filenames[c]) and (filename not in self.test_filenames[c]):\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                elif split == \"valid\":\n",
    "                    if filename in self.valid_filenames[c]:\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                elif split == \"test\":\n",
    "                    if filename in self.test_filenames[c]:\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid split name.\")\n",
    "\n",
    "        self.filepath_list = list()\n",
    "        self.label_list = list()\n",
    "        for cc, c in enumerate(self.get_classes()):\n",
    "            f_extension = sorted(list(self.class_to_file[c]))\n",
    "            l_extension = [cc for i in f_extension]\n",
    "            f_extension = [self.root_dir + \"dataset/\" + c + \"/\" + filename for filename in f_extension]\n",
    "            self.filepath_list.extend(f_extension)\n",
    "            self.label_list.extend(l_extension)\n",
    "        self.number_of_samples = len(self.filepath_list)\n",
    "    def __len__(self):\n",
    "        return self.number_of_samples\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        sample = np.zeros((16000, ), dtype=np.float32)\n",
    "\n",
    "        sample_file = self.filepath_list[idx]\n",
    "\n",
    "        sample_from_file = read(sample_file)[1]\n",
    "\n",
    "        sample[:sample_from_file.size] = sample_from_file\n",
    "        sample = sample.reshape((16000, ))\n",
    "\n",
    "        # It is better to stick to PyTorch functions\n",
    "        # I swapped the `librosaÂ´ mfcc resampling with this PyTorch alternative\n",
    "        # however, the parameters are the same\n",
    "        mfcc_transform = T.MFCC(\n",
    "            sample_rate=16000,\n",
    "            n_mfcc=20,\n",
    "            melkwargs={\n",
    "              'n_fft': 2048,\n",
    "              'hop_length': 512})\n",
    "\n",
    "        sample = torch.transpose(mfcc_transform(torch.tensor(sample)), 0, 1)\n",
    "\n",
    "        #sample = librosa.feature.mfcc(y=sample, sr=16000, hop_length=512, n_fft=2048).transpose().astype(np.float32)\n",
    "\n",
    "        # we apply the augmentations on the sample if it is defined\n",
    "        if self.transform:\n",
    "            sample = self.transform(samples=sample.reshape(32, 1, 20), sample_rate=16000)\n",
    "            sample = sample.reshape(32, 20)\n",
    "\n",
    "        label = self.label_list[idx]\n",
    "\n",
    "        return sample, label\n",
    "    \n",
    "\n",
    "    def get_classes(self):\n",
    "        return [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\",\"right\", \n",
    "                \"left\", \"up\", \"down\", \"forward\", \"backward\", \"yes\", \"no\", \"stop\", \"start\", \"enable\", \n",
    "                \"disable\", \"ok\", \"cancel\", \"open\", \"close\", \"zoom in\", \"zoom out\", \"previous\", \"next\", \n",
    "                \"send\", \"receive\", \"move\", \"rotate\", \"record\", \"enter\", \"digit\", \"direction\", \"options\", \"undo\"]\n",
    "\n",
    "    def get_valid_filenames(self):\n",
    "        class_names = self.get_classes()\n",
    "\n",
    "        class_to_filename = defaultdict(set)\n",
    "        with open(self.root_dir + \"dataset/val.txt\", \"r\") as fp:\n",
    "            for line in fp:\n",
    "                clean_line = line.strip().split(\"/\")\n",
    "\n",
    "                if clean_line[0] in class_names:\n",
    "                    class_to_filename[clean_line[0]].add(clean_line[1])\n",
    "\n",
    "        return class_to_filename\n",
    "\n",
    "    def get_test_filenames(self):\n",
    "        class_names = self.get_classes()\n",
    "\n",
    "        class_to_filename = defaultdict(set)\n",
    "        with open(self.root_dir + \"dataset/test.txt\", \"r\") as fp:\n",
    "            for line in fp:\n",
    "                clean_line = line.strip().split(\"/\")\n",
    "\n",
    "                if clean_line[0] in class_names:\n",
    "                    class_to_filename[clean_line[0]].add(clean_line[1])\n",
    "\n",
    "        return class_to_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6577e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import Compose, Gain, PolarityInversion\n",
    "\n",
    "# Initialize augmentation callable\n",
    "train_transforms = Compose(\n",
    "    transforms=[\n",
    "        Gain(\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        PolarityInversion(p=0.5)\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    transforms=[\n",
    "        Gain(\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        PolarityInversion(p=0.5)\n",
    "    ]\n",
    ")\n",
    "test_transforms = Compose(\n",
    "    transforms=[\n",
    "        Gain(\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        PolarityInversion(p=0.5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8704037",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder= os.path.join(os.getcwd(), \"Arabic_Speech_command/\")\n",
    "\n",
    "train_dataset = SpeechCommandsDataset(dataset_folder, \"train\")\n",
    "\n",
    "valid_dataset = SpeechCommandsDataset(dataset_folder, \"valid\")\n",
    "\n",
    "test_dataset  = SpeechCommandsDataset(dataset_folder, \"test\")\n",
    "\n",
    "### Augmented Datasets ###\n",
    "# Note we should concatenate both the non-augmented and the augmented training datasets\n",
    "\n",
    "train_dataset_aug = ConcatDataset([SpeechCommandsDataset(dataset_folder, \"train\", \n",
    "                                                         transform=train_transforms),\n",
    "                                                         train_dataset])\n",
    "\n",
    "valid_dataset_aug = SpeechCommandsDataset(dataset_folder, \"valid\", transform=val_transforms)\n",
    "\n",
    "test_dataset_aug  = SpeechCommandsDataset(dataset_folder, \"test\", transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0646cb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset, 7201 samples.\n",
      "Validation dataset, 2399 samples.\n",
      "Testing dataset, 2400 samples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset, {len(train_dataset)} samples.\")\n",
    "\n",
    "print(f\"Validation dataset, {len(valid_dataset)} samples.\")\n",
    "\n",
    "print(f\"Testing dataset, {len(test_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e4ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModel(nn.Module):\n",
    "    \"\"\"Neural network model (Transformer-based).\n",
    "\n",
    "    Args:\n",
    "        idim (int): Input feature dimension.\n",
    "        d_att (int): Attention dimension.\n",
    "        n_heads (int): The number of attention heads.\n",
    "        d_ff (int): Dimension of feed forward network.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "        n_layers (int): The number of encoder layers.\n",
    "        d_linear (int): Dimension of a hidden layer of the classifier.\n",
    "        n_classes (int): The number of the output classes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        idim=13,\n",
    "        d_att=64,\n",
    "        n_heads=4,\n",
    "        d_ff=512,\n",
    "        dropout_rate=0.1,\n",
    "        n_layers=3,\n",
    "        n_classes=40\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.subsampling = Subsampling(idim=idim, d_att=d_att)\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        self.encoder_layers = nn.Sequential()\n",
    "        for i in range(n_layers):\n",
    "            self.encoder_layers.add_module(\n",
    "                f'EncoderLayer{i}', \n",
    "                TransformerEncoderLayer(d_att, n_heads, d_ff, dropout_rate)\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(d_att)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.out = nn.Linear(d_att, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Recognize the input speech commands.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input features (batch, tmax, idim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Recognized classes (batch, num_classes).\n",
    "\n",
    "        \"\"\"   \n",
    "        # Transformer encoder\n",
    "        x = self.subsampling(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.encoder_layers(x)\n",
    "        x = self.norm(x)\n",
    "        # Classifier\n",
    "        x = torch.mean(x, dim=1)  # (b, t, d_att) -> (b, d_att)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"A Transformer encoder layer.\n",
    "\n",
    "    Args:\n",
    "        d_att (int): Attention dimension.\n",
    "        d_head (int): The number of attention heads.\n",
    "        d_ff (int): Dimension of feed forward network.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_att, d_head, d_ff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_att, d_head, dropout_rate)\n",
    "        self.ff = FeedForward(d_att, d_ff, dropout_rate)\n",
    "        self.norm_mha = nn.LayerNorm(d_att)\n",
    "        self.norm_ff = nn.LayerNorm(d_att)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Pre-encoded inputs (batch, tmax, d_att).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded outputs (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        # Multi-Head Attention\n",
    "        res = x\n",
    "        x = self.norm_mha(x)\n",
    "        x = res + self.dropout(self.mha(x, x, x))\n",
    "\n",
    "        # Feed-Forward\n",
    "        res = x\n",
    "        x = self.norm_ff(x)\n",
    "        x = res + self.dropout(self.ff(x))\n",
    "\n",
    "        return x\n",
    "class Subsampling(nn.Module):\n",
    "    \"\"\"Convolutional Subsampling.\n",
    "\n",
    "    Args:\n",
    "        idim (int): Input feature dimension.\n",
    "        d_att (int): Attention dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idim, d_att):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, d_att, kernel_size=(3, 3), stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(d_att, d_att, kernel_size=(3, 3), stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear = nn.Linear(256, d_att)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input features (batch, tmax, idim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Subsampled features (batch, tmax', d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)  # (b, t, idim) -> (b, c=1, t, idim)\n",
    "        x = self.conv(x)\n",
    "        b, c, t, f = x.size()\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = x.transpose(1, 2).contiguous().view(b, t, c * f)  # (b, c, t, f) -> (b, t, c * t)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding.\n",
    "\n",
    "    Args:\n",
    "        idim (int): Input feature dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "           super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x: (torch.Tensor): Subsampled features (batch, tmax, d_att).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded features (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        _, tmax, d_att = x.size()\n",
    "        pos = torch.arange(0, tmax, dtype=torch.float32).unsqueeze(1)\n",
    "        pe = torch.zeros(1, tmax, d_att, dtype=torch.float32).to(x.device)\n",
    "        pe[:, :, 0::2] = torch.sin(pos / torch.pow(10000, torch.arange(0, d_att, 2) / d_att))\n",
    "        pe[:, :, 1::2] = torch.cos(pos / torch.pow(10000, torch.arange(0, d_att, 2) / d_att))\n",
    "        x = x + pe\n",
    "\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention.\n",
    "\n",
    "    Args:\n",
    "        d_att (int): Dimension of attention.\n",
    "        d_head (int): The number of attention heads.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_att, n_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.linear_q = nn.Linear(d_att, d_att)\n",
    "        self.linear_k = nn.Linear(d_att, d_att)\n",
    "        self.linear_v = nn.Linear(d_att, d_att)\n",
    "        self.linear_head = nn.Linear(d_att, d_att)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_comn = d_att // self.n_heads\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            q: (torch.Tensor): Query (batch, tmax, d_att).\n",
    "            k: (torch.Tensor): Key (batch, tmax, d_att).\n",
    "            v: (torch.Tensor): Value (batch, tmax, d_att).\n",
    "            Returns:\n",
    "            torch.Tensor: Output shape (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        # Linear\n",
    "        qw = self.linear_q(q)\n",
    "        kw = self.linear_k(k)\n",
    "        vw = self.linear_v(v)\n",
    "\n",
    "        # Reshape tensor (b, t, d_att) -> (b, n_heads, t, d_comn)\n",
    "        b, t, d_att = q.size()\n",
    "        qw = qw.view(b, t, self.n_heads, self.d_comn).transpose(1, 2)\n",
    "        kw = kw.view(b, t, self.n_heads, self.d_comn).transpose(1, 2)\n",
    "        vw = vw.view(b, t, self.n_heads, self.d_comn).transpose(1, 2)\n",
    "\n",
    "        # Dot-attention\n",
    "        matmul = torch.matmul(qw, kw.transpose(2, 3)) \n",
    "        scale = matmul / torch.sqrt(torch.tensor(self.d_comn))\n",
    "        softmax = torch.softmax(scale, dim=-1)\n",
    "        att = torch.matmul(self.dropout(softmax), vw)  # (b, n_heads, t, d_comn)\n",
    "\n",
    "        # Concatenate\n",
    "        att = att.transpose(1, 2).contiguous().view(b, -1, self.n_heads * self.d_comn)  # (b, t, d_att)\n",
    "\n",
    "        # Linear\n",
    "        mha = self.linear_head(att)\n",
    "\n",
    "        return mha\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-Forward Network.\n",
    "\n",
    "    Args:\n",
    "        d_ff (int): Dimension of feed-forward network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_att, d_ff, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_att, d_ff), nn.ReLU(), nn.Dropout(dropout_rate), nn.Linear(d_ff, d_att)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x: (torch.Tensor): Input shape (batch, tmax, d_att).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output shape (batch, tmax, d_att).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ee3a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some metric lists\n",
    "def train(config, \n",
    "          train_set, \n",
    "          val_set, \n",
    "          results_dir='./results',\n",
    "          continue_training=False,\n",
    "          epochs=5, \n",
    "          plot=False):\n",
    "    \n",
    "    model = NeuralNetworkModel(dropout_rate=config[\"dropout_rate\"], \n",
    "                               n_heads=config[\"n_heads\"]).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    \n",
    "    if continue_training:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(results_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    \n",
    "    #store evaluation metrics in these lists\n",
    "    train_acc_list = []\n",
    "    train_loss_list = []\n",
    "    dev_acc_list = []\n",
    "    dev_loss_list = []\n",
    "    \n",
    "    # best loss tracking for model checkpointing\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        batch_train_loss = 0\n",
    "        batch_train_acc = 0\n",
    "        batch_dev_loss = 0\n",
    "        batch_dev_acc = 0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for feats, labels in train_loader:\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_train_loss += loss.item()\n",
    "            batch_train_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "\n",
    "        train_loss_list.append(batch_train_loss / len(train_loader))\n",
    "        train_acc_list.append(batch_train_acc / len(train_loader.dataset))\n",
    "        # Validation\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for feats, labels in valid_loader:\n",
    "                feats, labels = feats.to(device), labels.to(device)\n",
    "\n",
    "                # Forward\n",
    "                outputs = model(feats)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                batch_dev_loss += loss.item()\n",
    "                batch_dev_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "\n",
    "        dev_loss_list.append(batch_dev_loss / len(valid_loader))\n",
    "        dev_acc_list.append(batch_dev_acc / len(valid_loader.dataset))\n",
    "        \n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        torch.save((model.state_dict(), optimizer.state_dict()), os.path.join(results_dir, f\"checkpoint{epoch}.pt\"))\n",
    "        \n",
    "        checkpoint = Checkpoint.from_directory(results_dir)\n",
    "        session.report({\n",
    "                        \"loss\" : dev_loss_list[-1],\n",
    "                        \"accuracy\" : dev_acc_list[-1]\n",
    "                        },\n",
    "                        checkpoint=checkpoint)\n",
    "        \n",
    "        # plot results if flag is True\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 6.5))\n",
    "\n",
    "            ax[0].plot(range(len(train_loss_list)), train_loss_list, label='train loss')\n",
    "            ax[0].plot(range(len(dev_loss_list)), dev_loss_list, label='val loss')\n",
    "            ax[0].set_title('Loss Evolution')\n",
    "\n",
    "            ax[1].plot(range(len(train_acc_list)), train_acc_list, label='train accuracy')\n",
    "            ax[1].plot(range(len(dev_acc_list)), dev_acc_list, label='val accuracy')\n",
    "            ax[1].set_title('Accuracy Evolution')\n",
    "\n",
    "            ax[0].legend()\n",
    "            ax[1].legend()\n",
    "\n",
    "            plt.show()\n",
    "        \n",
    "def test(model, test_set, device, batch_size=64):\n",
    "    best_loss = float(\"inf\")\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Test\n",
    "    test_start_time = time.time()\n",
    "    \n",
    "    batch_test_loss = 0\n",
    "    batch_test_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feats, labels in test_loader:\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(feats)\n",
    "            batch_test_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "\n",
    "    test_mean_acc = batch_test_acc / len(test_loader.dataset)\n",
    "\n",
    "    # Print out test time\n",
    "    test_time = time.time() - test_start_time\n",
    "    print(f\">> Test Time: {test_time}\")\n",
    "    \n",
    "    return test_mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f8df0",
   "metadata": {},
   "source": [
    "# Train/Validation Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f123987",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-25 11:08:49</td></tr>\n",
       "<tr><td>Running for: </td><td>00:45:40.91        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.6/7.6 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=9<br>Bracket: Iter 32.000: -1.3152421963842291 | Iter 16.000: -1.3266912306609906 | Iter 8.000: -1.3281252666523584 | Iter 4.000: -1.63234743946477 | Iter 2.000: -2.2662609054749474 | Iter 1.000: -3.181367309469926<br>Resources requested: 8.0/12 CPUs, 1.0/1 GPUs, 0.0/2.31 GiB heap, 0.0/1.16 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  ... 6 more trials not shown (6 PENDING)\n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  n_heads</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_a8179_00009</td><td>RUNNING   </td><td>139.174.120.12:7481 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.102795</td><td style=\"text-align: right;\">0.00336029 </td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">2.90575</td><td style=\"text-align: right;\"> 0.200083 </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_a8179_00010</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.135188</td><td style=\"text-align: right;\">0.000266807</td><td style=\"text-align: right;\">        4</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>train_a8179_00011</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.140463</td><td style=\"text-align: right;\">0.00011437 </td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>train_a8179_00012</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.123504</td><td style=\"text-align: right;\">0.00109548 </td><td style=\"text-align: right;\">        4</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>train_a8179_00013</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.116288</td><td style=\"text-align: right;\">0.00145061 </td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>train_a8179_00014</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.10958 </td><td style=\"text-align: right;\">0.00585772 </td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>train_a8179_00015</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.148593</td><td style=\"text-align: right;\">0.00554738 </td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>train_a8179_00016</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.130147</td><td style=\"text-align: right;\">0.000417869</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>train_a8179_00017</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.117254</td><td style=\"text-align: right;\">0.00521254 </td><td style=\"text-align: right;\">        2</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>train_a8179_00018</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.114377</td><td style=\"text-align: right;\">0.000275667</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>train_a8179_00019</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.106665</td><td style=\"text-align: right;\">0.000316766</td><td style=\"text-align: right;\">        4</td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">                    </td></tr>\n",
       "<tr><td>train_a8179_00000</td><td>TERMINATED</td><td>139.174.120.12:30338</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.112691</td><td style=\"text-align: right;\">0.00013671 </td><td style=\"text-align: right;\">        4</td><td style=\"text-align: right;\">1.03222</td><td style=\"text-align: right;\"> 0.737807 </td><td style=\"text-align: right;\">                  50</td></tr>\n",
       "<tr><td>train_a8179_00001</td><td>TERMINATED</td><td>139.174.120.12:11423</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.104264</td><td style=\"text-align: right;\">0.00141646 </td><td style=\"text-align: right;\">       32</td><td style=\"text-align: right;\">1.51917</td><td style=\"text-align: right;\"> 0.692789 </td><td style=\"text-align: right;\">                  32</td></tr>\n",
       "<tr><td>train_a8179_00002</td><td>TERMINATED</td><td>139.174.120.12:19554</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.101688</td><td style=\"text-align: right;\">0.000131853</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">3.68595</td><td style=\"text-align: right;\"> 0.0304293</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_a8179_00003</td><td>TERMINATED</td><td>139.174.120.12:19872</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.109131</td><td style=\"text-align: right;\">0.00201761 </td><td style=\"text-align: right;\">        4</td><td style=\"text-align: right;\">1.52538</td><td style=\"text-align: right;\"> 0.731555 </td><td style=\"text-align: right;\">                  50</td></tr>\n",
       "<tr><td>train_a8179_00004</td><td>TERMINATED</td><td>139.174.120.12:732  </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      0.112918</td><td style=\"text-align: right;\">0.00132745 </td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">1.35881</td><td style=\"text-align: right;\"> 0.695707 </td><td style=\"text-align: right;\">                  16</td></tr>\n",
       "<tr><td>train_a8179_00005</td><td>TERMINATED</td><td>139.174.120.12:5491 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.114089</td><td style=\"text-align: right;\">0.000194003</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">3.5599 </td><td style=\"text-align: right;\"> 0.0637766</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_a8179_00006</td><td>TERMINATED</td><td>139.174.120.12:5920 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.123353</td><td style=\"text-align: right;\">0.000384268</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">3.59668</td><td style=\"text-align: right;\"> 0.0516882</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_a8179_00007</td><td>TERMINATED</td><td>139.174.120.12:6303 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      0.128249</td><td style=\"text-align: right;\">0.000515754</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">2.25204</td><td style=\"text-align: right;\"> 0.352647 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>train_a8179_00008</td><td>TERMINATED</td><td>139.174.120.12:6882 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      0.104921</td><td style=\"text-align: right;\">0.00845565 </td><td style=\"text-align: right;\">       32</td><td style=\"text-align: right;\">2.31912</td><td style=\"text-align: right;\"> 0.356398 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 10:23:06,367\tINFO worker.py:1550 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th style=\"text-align: right;\">  accuracy</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">   loss</th><th>node_ip       </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_a8179_00000</td><td style=\"text-align: right;\"> 0.737807 </td><td>2023-03-25_10-37-18</td><td>True  </td><td>                </td><td>cc7629795ca94f94868caa0a82b689e5</td><td>mbe19     </td><td style=\"text-align: right;\">                        50</td><td style=\"text-align: right;\">1.03222</td><td>139.174.120.12</td><td style=\"text-align: right;\">30338</td><td>True               </td><td style=\"text-align: right;\">            846.637 </td><td style=\"text-align: right;\">           17.1035</td><td style=\"text-align: right;\">      846.637 </td><td style=\"text-align: right;\"> 1679737038</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  50</td><td>a8179_00000</td><td style=\"text-align: right;\">   0.00304747</td></tr>\n",
       "<tr><td>train_a8179_00001</td><td style=\"text-align: right;\"> 0.692789 </td><td>2023-03-25_10-46-37</td><td>True  </td><td>                </td><td>690dcca6431b4e3aa71333d777ea9d09</td><td>mbe19     </td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">1.51917</td><td>139.174.120.12</td><td style=\"text-align: right;\">11423</td><td>True               </td><td style=\"text-align: right;\">            553.659 </td><td style=\"text-align: right;\">           17.412 </td><td style=\"text-align: right;\">      553.659 </td><td style=\"text-align: right;\"> 1679737597</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  32</td><td>a8179_00001</td><td style=\"text-align: right;\">   0.00311875</td></tr>\n",
       "<tr><td>train_a8179_00002</td><td style=\"text-align: right;\"> 0.0304293</td><td>2023-03-25_10-47-01</td><td>True  </td><td>                </td><td>fc9b4f1aa1594028982d9bb5e864a64a</td><td>mbe19     </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">3.68595</td><td>139.174.120.12</td><td style=\"text-align: right;\">19554</td><td>True               </td><td style=\"text-align: right;\">             19.3095</td><td style=\"text-align: right;\">           19.3095</td><td style=\"text-align: right;\">       19.3095</td><td style=\"text-align: right;\"> 1679737621</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>a8179_00002</td><td style=\"text-align: right;\">   0.00309086</td></tr>\n",
       "<tr><td>train_a8179_00003</td><td style=\"text-align: right;\"> 0.731555 </td><td>2023-03-25_11-01-25</td><td>True  </td><td>                </td><td>8f9366132bce428b8309602be2466b14</td><td>mbe19     </td><td style=\"text-align: right;\">                        50</td><td style=\"text-align: right;\">1.52538</td><td>139.174.120.12</td><td style=\"text-align: right;\">19872</td><td>True               </td><td style=\"text-align: right;\">            860.118 </td><td style=\"text-align: right;\">           17.2755</td><td style=\"text-align: right;\">      860.118 </td><td style=\"text-align: right;\"> 1679738485</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  50</td><td>a8179_00003</td><td style=\"text-align: right;\">   0.00294733</td></tr>\n",
       "<tr><td>train_a8179_00004</td><td style=\"text-align: right;\"> 0.695707 </td><td>2023-03-25_11-06-13</td><td>True  </td><td>                </td><td>5628ef452928423983619afb6bd78fd5</td><td>mbe19     </td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">1.35881</td><td>139.174.120.12</td><td style=\"text-align: right;\">  732</td><td>True               </td><td style=\"text-align: right;\">            282.774 </td><td style=\"text-align: right;\">           17.8081</td><td style=\"text-align: right;\">      282.774 </td><td style=\"text-align: right;\"> 1679738773</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  16</td><td>a8179_00004</td><td style=\"text-align: right;\">   0.00345778</td></tr>\n",
       "<tr><td>train_a8179_00005</td><td style=\"text-align: right;\"> 0.0637766</td><td>2023-03-25_11-06-36</td><td>True  </td><td>                </td><td>5ee0b78d29694a4baa14fa2cb44e5229</td><td>mbe19     </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">3.5599 </td><td>139.174.120.12</td><td style=\"text-align: right;\"> 5491</td><td>True               </td><td style=\"text-align: right;\">             19.5659</td><td style=\"text-align: right;\">           19.5659</td><td style=\"text-align: right;\">       19.5659</td><td style=\"text-align: right;\"> 1679738796</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>a8179_00005</td><td style=\"text-align: right;\">   0.00290728</td></tr>\n",
       "<tr><td>train_a8179_00006</td><td style=\"text-align: right;\"> 0.0516882</td><td>2023-03-25_11-07-00</td><td>True  </td><td>                </td><td>a759e89120ff4ba2a1abac0bb75e85de</td><td>mbe19     </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">3.59668</td><td>139.174.120.12</td><td style=\"text-align: right;\"> 5920</td><td>True               </td><td style=\"text-align: right;\">             19.0217</td><td style=\"text-align: right;\">           19.0217</td><td style=\"text-align: right;\">       19.0217</td><td style=\"text-align: right;\"> 1679738820</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>a8179_00006</td><td style=\"text-align: right;\">   0.00307059</td></tr>\n",
       "<tr><td>train_a8179_00007</td><td style=\"text-align: right;\"> 0.352647 </td><td>2023-03-25_11-07-41</td><td>True  </td><td>                </td><td>7b031f1138f3498f9626ae5a47ae8c2f</td><td>mbe19     </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">2.25204</td><td>139.174.120.12</td><td style=\"text-align: right;\"> 6303</td><td>True               </td><td style=\"text-align: right;\">             37.3963</td><td style=\"text-align: right;\">           17.7016</td><td style=\"text-align: right;\">       37.3963</td><td style=\"text-align: right;\"> 1679738861</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>a8179_00007</td><td style=\"text-align: right;\">   0.00297427</td></tr>\n",
       "<tr><td>train_a8179_00008</td><td style=\"text-align: right;\"> 0.356398 </td><td>2023-03-25_11-08-20</td><td>True  </td><td>                </td><td>459205b3c1f046659a1c4acb189fc59e</td><td>mbe19     </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">2.31912</td><td>139.174.120.12</td><td style=\"text-align: right;\"> 6882</td><td>True               </td><td style=\"text-align: right;\">             35.7536</td><td style=\"text-align: right;\">           16.7935</td><td style=\"text-align: right;\">       35.7536</td><td style=\"text-align: right;\"> 1679738900</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>a8179_00008</td><td style=\"text-align: right;\">   0.00290608</td></tr>\n",
       "<tr><td>train_a8179_00009</td><td style=\"text-align: right;\"> 0.200083 </td><td>2023-03-25_11-08-44</td><td>False </td><td>                </td><td>42f018b4f9ae452187bbee392a6541ce</td><td>mbe19     </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.90575</td><td>139.174.120.12</td><td style=\"text-align: right;\"> 7481</td><td>True               </td><td style=\"text-align: right;\">             19.763 </td><td style=\"text-align: right;\">           19.763 </td><td style=\"text-align: right;\">       19.763 </td><td style=\"text-align: right;\"> 1679738924</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>a8179_00009</td><td style=\"text-align: right;\">   0.0028522 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_samples=500\n",
    "max_num_epochs=50\n",
    "\n",
    "config = {\n",
    "    \"dropout_rate\": tune.loguniform(1e-1, 1.5e-1),\n",
    "    \"n_heads\": tune.choice([2, 4, 8, 16, 32]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"batch_size\": tune.choice([32, 64, 128])\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "\n",
    "result = tune.run(\n",
    "            partial(train, \n",
    "                    train_set=train_dataset_aug, \n",
    "                    val_set=valid_dataset_aug,\n",
    "                    epochs=max_num_epochs),\n",
    "            resources_per_trial={\"cpu\": 8, \"gpu\": 1},\n",
    "            config=config,\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "            progress_reporter=reporter)\n",
    "\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "\n",
    "best_trained_model = NeuralNetworkModel(dropout_rate=best_trial.config[\"dropout_rate\"], \n",
    "                                        n_heads=best_trial.config[\"n_heads\"])\n",
    "\n",
    "best_trained_model.to(device)\n",
    "\n",
    "best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric=\"loss\", mode=\"min\")\n",
    "best_checkpoint_dir = best_checkpoint.to_directory(path=\"results\")\n",
    "\n",
    "model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint.pt\"))\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "test_acc = test(model=best_trained_model, test_set=test_dataset,\n",
    "                device=device, batch_size=best_trial.config[\"batch_size\"])\n",
    "\n",
    "print(f\"Best trial test set accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Data (augmented) with Validation Data (Augmented)\n",
    "# We can notice that the max accuracy is reaching a value of ~ 77%\n",
    "\n",
    "# Build a model\n",
    "model = NeuralNetworkModel(dropout_rate=best_trial.config[\"dropout_rate\"], \n",
    "                               n_heads=best_trial.config[\"n_heads\"]).to(device)\n",
    "\n",
    "#best_trial.config[\"lr\"]\n",
    "#best_trial.config[\"batch_size\"]\n",
    "        \n",
    "print(f\"# model parameters: {sum(p.numel() for p in aug_model.parameters()):,}\")\n",
    "\n",
    "# Define an optimizer and a loss function\n",
    "optimizer = optim.Adam(aug_model.parameters(), lr=best_trial.config[\"lr\"])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train(, optimizer=optimizer, \n",
    "      train_loader=train_loader_aug, valid_loader=valid_loader_aug, \n",
    "      criterion=aug_loss_fn, epochs=epochs, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2429c4d",
   "metadata": {},
   "source": [
    "# Test Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the model with the test augmented data\n",
    "\n",
    "test(model=aug_model, test_loader=test_loader_aug, criterion=aug_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a331d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the model with the test augmented data\n",
    "\n",
    "test(model=baseline_model, test_loader=test_loader_aug, criterion=baseline_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the model with the test non-augmented data\n",
    "\n",
    "test(model=aug_model, test_loader=test_loader, criterion=aug_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the model with the test non-augmented data\n",
    "\n",
    "test(model=baseline_model, test_loader=test_loader, criterion=baseline_loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
